# -*- coding: utf-8 -*-
"""Cornell Chatbot.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gimHjwwl1B6aeK2dkfvUll-UEXi-pqw1
"""

import string
import re
from numpy import array, argmax, random, take
import pandas as pd
from keras.models import Sequential
from keras.layers import Dense, LSTM, Embedding, RepeatVector, Dropout
from keras.preprocessing.text import Tokenizer
from keras.callbacks import ModelCheckpoint
from keras.preprocessing.sequence import pad_sequences
from keras.models import load_model
from keras import optimizers
import matplotlib.pyplot as plt
pd.set_option('display.max_colwidth', 200)

movie_lines_features = ["LineID", "Character", "Movie", "Name", "Line"]
movie_lines = pd.read_csv("movie_lines.txt", sep = "\+\+\+\$\+\+\+", engine = "python", index_col = False, names = movie_lines_features)
movie_lines = movie_lines[["LineID", "Line"]]

movie_lines.head()

movie_line_dict = {}

for i in range(movie_lines.shape[0]):
    key = int(movie_lines.loc[i].LineID[1:-1])
    value = movie_lines.loc[i].Line
    movie_line_dict[key] = value

movie_conversations_features = ["Character1", "Character2", "Movie", "Conversation"]
movie_conversations = pd.read_csv("movie_conversations.txt", sep = "\+\+\+\$\+\+\+", engine = "python", index_col = False, names = movie_conversations_features)

movie_conversations = movie_conversations["Conversation"]

movie_conversations.head()

movie_conversations_list = []
for row in range(movie_conversations.shape[0]):
    new_list = []
    for i in movie_conversations[row][2:].split():
        new_list.append(int(i[2:len(i)-2]))
    movie_conversations_list.append(new_list)

dialogue_1 = []
dialogue_2 = []

for convo in movie_conversations_list:
    for j in range(len(convo)-1):
        dialogue_1.append(movie_line_dict[convo[j]])
        dialogue_2.append(movie_line_dict[convo[j+1]])

import re
dialogue_1 = [re.sub(r'^\d+', '', str(sentence)) for sentence in dialogue_1]
dialogue_2 = [re.sub(r'^\d+', '', str(sentence)) for sentence in dialogue_2]

dialogue_1 = [re.sub(r'[^\w\s]', '', sentence).lower() for sentence in dialogue_1]
dialogue_2 = [re.sub(r'[^\w\s]', '', sentence).lower() for sentence in dialogue_2]

from keras.preprocessing.text import Tokenizer

def tokenization(lines):
      tokenizer = Tokenizer()
      tokenizer.fit_on_texts(lines)
      return tokenizer

tokenizer = tokenization(dialogue_1 + dialogue_2)

len(tokenizer.word_index)

dialogue_1 = dialogue_1[:len(dialogue_1)//8]
dialogue_2 = dialogue_2[:len(dialogue_2)//8]

from keras.preprocessing.sequence import pad_sequences
def encode_sequences(tokenizer, length, lines):
         # integer encode sequences
         seq = tokenizer.texts_to_sequences(lines)
         # pad sequences with 0 values
         seq = pad_sequences(seq, maxlen=length, padding='post')
         return seq

train_X = encode_sequences(tokenizer, 15, dialogue_1)

train_Y = encode_sequences(tokenizer, 15, dialogue_2)


def define_model(in_vocab,out_vocab, in_timesteps,out_timesteps,units):
      model = Sequential()
      model.add(Embedding(in_vocab, units, input_length=in_timesteps, mask_zero=True))
      model.add(LSTM(units))
      model.add(RepeatVector(out_timesteps))
      model.add(LSTM(units, return_sequences=True))
      model.add(Dense(out_vocab, activation='softmax'))
      return model

n = len(tokenizer.word_index) + 1
# model compilation
model = define_model(n, n, 15, 15, 1024)

rms = optimizers.RMSprop(lr=0.01)
model.compile(optimizer=rms, loss='sparse_categorical_crossentropy')

# train model
history = model.fit(train_X, train_Y.reshape(train_Y.shape[0], train_Y.shape[1], 1), epochs= 70, verbose=1)

plt.plot(history.history['loss'])
plt.show()

filename = 'chatbot_1_epochs.h5'
model.save(filename)

# preprocessing function
def preprocess(sentence):
    sentence = re.sub(r'^\d+', '', str(sentence))
    sentence = re.sub(r'[^\w\s]', '', sentence).lower()
    return sentence

# function to get german words using tokens
def get_word(n, tokenizer):
      for word, index in tokenizer.word_index.items():
          if index == n:
              return word
      return None

#function to form sentences in deutsch using tokens 
def get_sentence(word_list):
    final = ''
    for i in word_list:
        if i != 0:
            word = get_word(i, tokenizer)
            final = final + str(word) + ' '
    return final

def chat(sentence):
    sentence = preprocess(sentence)
    a = encode_sequences(tokenizer, 15, [sentence])
    b = model.predict_classes(a)[0]
    translation = get_sentence(b)
    return translation

chat("What did he say?")

chat('how long have you been waiting for me')

chat("I hope this works")

chat("It is strange")

chat("It was nice to see you")

chat("I know")

chat("She Okay? ")

